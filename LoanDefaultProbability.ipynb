{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Loan Default Risk \n",
    "\n",
    "https://www.kaggle.com/c/home-credit-default-risk\n",
    "\n",
    "Predict the probabilty an applicant will default on a loan.\n",
    "\n",
    "The main source is an application (identified by a unique SK_ID_CURR). The main source is imported as train and test. There are 6 other data sources at differing grains. Each source is imported and then aligned to the grain of one application. In order to align, a dataframe named df_all_ids is first created with all the SK_ID_CURR of that source. When preprocessing and data engineering is complete for each source, the dataframe df_all_ids is joined to the primary dataframes of train and test. The process then begins for the next source.  Once all the sources are imported, the primary dataframes are copied and renamed train_X and test_X. The columns are aligned and features are scaled. LightGBM with crossfold validation is the algorithm used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import gc\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold \n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Automatically give the time taken to execute each cell\n",
    "# Install with pip install ipython-autotime \n",
    "%load_ext autotime\n",
    "\n",
    "# Plotting Graphs Configuration\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = 14, 7\n",
    "plt.style.use('seaborn-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/application_train.csv')\n",
    "test = pd.read_csv('data/application_test.csv')\n",
    "traintarget = train[['SK_ID_CURR', 'TARGET']].copy()\n",
    "\n",
    "train.set_index(\"SK_ID_CURR\", inplace = True)\n",
    "test.set_index(\"SK_ID_CURR\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def app_preprocessing(df):\n",
    "    \"\"\"drop_these_columns = [ 'WALLSMATERIAL_MODE','ORGANIZATION_TYPE', 'WALLSMATERIAL_MODE', \n",
    "          'REGION_RATING_CLIENT', 'HOUSETYPE_MODE', 'CNT_CHILDREN', 'OBS_60_CNT_SOCIAL_CIRCLE', \n",
    "          'DEF_60_CNT_SOCIAL_CIRCLE', LIVE_CITY_NOT_WORK_CITY', 'LIVE_REGION_NOT_WORK_REGION', \n",
    "          'AMT_REQ_CREDIT_BUREAU_HOUR','AMT_REQ_CREDIT_BUREAU_WEEK', 'AMT_REQ_CREDIT_BUREAU_DAY',\n",
    "          'FLAG_CONT_MOBILE', 'FLAG_MOBIL', 'FLAG_EMAIL', 'FLAG_PHONE', 'NAME_TYPE_SUITE',\n",
    "          'NAME_INCOME_TYPE', 'FLAG_OWN_CAR', 'REG_REGION_NOT_WORK_REGION', 'EMERGENCYSTATE_MODE',\n",
    "          'REG_REGION_NOT_LIVE_REGION', 'WEEKDAY_APPR_PROCESS_START',\n",
    "          'FLAG_EMP_PHONE', 'REG_CITY_NOT_WORK_CITY']  \n",
    "    df.drop(drop_these_columns, axis=1, inplace=True)\n",
    "    \"\"\"\n",
    "    \n",
    "    df.fillna({ 'EMERGENCYSTATE_MODE': 'No',\n",
    "                    'OCCUPATION_TYPE': 'unknown',\n",
    "                    'NAME_TYPE_SUITE': 'unknown',\n",
    "                    'CNT_FAM_MEMBERS': 1, \n",
    "                    'EXT_SOURCE_1': df['EXT_SOURCE_1'].median(),\n",
    "                    'EXT_SOURCE_2': df['EXT_SOURCE_2'].median(),\n",
    "                    'EXT_SOURCE_3': df['EXT_SOURCE_3'].median(),\n",
    "                    'OWN_CAR_AGE': df['OWN_CAR_AGE'].median(), \n",
    "                    'AMT_GOODS_PRICE': df['AMT_GOODS_PRICE'].median(), \n",
    "                    'AMT_ANNUITY': df['AMT_ANNUITY'].median(), \n",
    "                    'TOTALAREA_MODE': df['TOTALAREA_MODE'].median()   \n",
    "                     }, inplace=True)\n",
    "    df['NAME_EDUCATION_TYPE'].replace({'Higher education': 'Higher',\n",
    "                                     'Academic degree': 'Higher',\n",
    "                                     'Incomplete higher':'Secondary',\n",
    "                                     'Lower secondary': 'Secondary',\n",
    "                                     'Secondary / secondary special':'Secondary'}, inplace=True)\n",
    "    df['CODE_GENDER'].replace('XNA', 'F', inplace=True) \n",
    "    \n",
    "    df['DAYS_EMPLOYED_ANOM'] = df[\"DAYS_EMPLOYED\"] == 365243\n",
    "    df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace= True)\n",
    "    df['NAME_FAMILY_STATUS'].replace('Unknown', 'Married', inplace=True)\n",
    "  \n",
    "    return df\n",
    "\n",
    "train = app_preprocessing(train)\n",
    "test = app_preprocessing(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "for col in train: \n",
    "    if train[col].dtype == 'object':\n",
    "        if len(list(train[col].unique())) <= 2:          \n",
    "            le.fit(train[col])  \n",
    "            train[col] = le.transform(train[col]) \n",
    "            test[col] = le.transform(test[col]) \n",
    "\n",
    "def one_hot_encoder(df):\n",
    "    original_columns = list(df.columns)\n",
    "    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    df = pd.get_dummies(df, columns= categorical_columns)\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, new_columns\n",
    "\n",
    "train, train_cat = one_hot_encoder(train)\n",
    "test, test_cat = one_hot_encoder(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fill_na_with(df, strategy_ = 'median'):\n",
    "    imputer = Imputer(strategy = strategy_, axis = 0)\n",
    "    return pd.DataFrame(imputer.fit_transform(df), columns = df.columns, index = df.index)\n",
    "\n",
    "train = fill_na_with(train)\n",
    "test = fill_na_with(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def app_feature_engineering(df):    \n",
    "    df['EMPLOYED_TO_BIRTH_RATIO'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH'] \n",
    "    df['CREDIT_TO_INCOME_RATIO'] =  df['AMT_CREDIT'] / df['AMT_INCOME_TOTAL'] \n",
    "    df['CREDIT_TO_ANNUITY_RATIO'] =  df['AMT_CREDIT'] / df['AMT_ANNUITY']\n",
    "    df['CREDIT_TO_GOODS_RATIO'] =  df['AMT_CREDIT'] / df['AMT_GOODS_PRICE']\n",
    "    df['ANNUITY_TO_INCOME_RATIO'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL'] \n",
    "    df['INCOME_PERPERSON'] = df['AMT_INCOME_TOTAL'] / df['CNT_FAM_MEMBERS']  \n",
    "    df['CREDIT_PERPERSON'] = df['AMT_CREDIT'] / df['CNT_FAM_MEMBERS']   \n",
    "    df['INCOME_MINUS_CREDIT_PERPERSON'] = (df['AMT_INCOME_TOTAL'] - df['AMT_CREDIT']) / df['CNT_FAM_MEMBERS']  \n",
    "    df['CARAGE_TO_BIRTH_RATIO'] = df['OWN_CAR_AGE']  / df['DAYS_BIRTH'] \n",
    "    df['PHONETO_BIRTH_RATIO'] = df['DAYS_LAST_PHONE_CHANGE']  / df['DAYS_BIRTH'] \n",
    "    df['OBS_30_DEFAULT_RATIO'] = df ['DEF_30_CNT_SOCIAL_CIRCLE'] / df['OBS_30_CNT_SOCIAL_CIRCLE']\n",
    "    \n",
    "app_feature_engineering(train)\n",
    "app_feature_engineering (test)\n",
    "\n",
    "train = fill_na_with(train)\n",
    "test = fill_na_with(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Data Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Source: Bureau and Bureau Balance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bureau = pd.read_csv('data/bureau.csv')\n",
    "bureau = bureau.add_prefix('bureau_') #add prefix showing source\n",
    "bureau = bureau.rename(columns = {'bureau_SK_ID_CURR': 'SK_ID_CURR', 'bureau_SK_ID_BUREAU': 'SK_ID_BUREAU',}) #remove prefix from id\n",
    "bureau = bureau.fillna(0) \n",
    "\n",
    "bb = pd.read_csv('data/bureau_balance.csv')\n",
    "bb = bb.add_prefix('bb_') #add prefix showing source\n",
    "bb = bb.rename(columns = { 'bb_SK_ID_BUREAU': 'SK_ID_BUREAU',}) #remove prefix from id\n",
    "bb = bb.fillna(0)\n",
    "\n",
    "df_all_ids = pd.DataFrame({'SK_ID_CURR':bureau['SK_ID_CURR'].unique()}) #used to align grain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# align to grain of SK_ID_BUREAU\n",
    "bb, bb_cat = one_hot_encoder(bb)\n",
    "\n",
    "grp = bb.groupby(['SK_ID_BUREAU']).agg({\n",
    "        'bb_MONTHS_BALANCE': ['size'],\n",
    "        'bb_STATUS_X': ['sum'],\n",
    "        'bb_STATUS_C': ['sum'],\n",
    "        'bb_STATUS_1': ['sum']\n",
    "})\n",
    "grp.columns = pd.Index([e[0] + '_' + e[1] for e in grp.columns.tolist()])\n",
    "bureau = bureau.join(grp, on = ['SK_ID_BUREAU'], how = 'left') \n",
    "\n",
    "# formulas on grain of SK_ID_BUREAU\n",
    "bureau['bb_STATUS_X_PERCENT'] = bureau['bb_STATUS_X_sum'] / bureau['bb_MONTHS_BALANCE_size']\n",
    "bureau['bb_STATUS_C_PERCENT'] = bureau['bb_STATUS_C_sum'] / bureau['bb_MONTHS_BALANCE_size']\n",
    "bureau['bb_STATUS_1_PERCENT'] = bureau['bb_STATUS_1_sum'] / bureau['bb_MONTHS_BALANCE_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "groupby_SK_ID_CURR = bureau.groupby(by=['SK_ID_CURR'])\n",
    "\n",
    "# created at level of bureau\n",
    "bureau['bureau_CREDIT_ACTIVE_FLAG'] = (bureau['bureau_CREDIT_ACTIVE'] != 'Closed').astype(int)\n",
    "bureau['bureau_CREDIT_ENDDATE_FLAG'] = (bureau['bureau_DAYS_CREDIT_ENDDATE'] > 0).astype(int)\n",
    "\n",
    "grp = groupby_SK_ID_CURR['bureau_DAYS_CREDIT'].agg('count').reset_index()\n",
    "grp.rename(index=str, columns={'bureau_DAYS_CREDIT': 'bureau_NUMBER_PAST_LOANS'},inplace=True)\n",
    "df_all_ids = df_all_ids.merge(grp, on=['SK_ID_CURR'], how='left')\n",
    "\n",
    "grp = groupby_SK_ID_CURR['bureau_CREDIT_TYPE'].agg('nunique').reset_index()\n",
    "grp.rename(index=str, columns={'bureau_CREDIT_TYPE': 'bureau_NUMBER_LOAN_TYPES'},inplace=True)\n",
    "df_all_ids = df_all_ids.merge(grp, on=['SK_ID_CURR'], how='left')\n",
    "df_all_ids = fill_na_with(df_all_ids)\n",
    "\n",
    "df_all_ids['bureau_PASTLOANS_PER_TYPE_RATIO'] = df_all_ids['bureau_NUMBER_PAST_LOANS'] / df_all_ids['bureau_NUMBER_LOAN_TYPES']\n",
    "\n",
    "bureau, cat_cols = one_hot_encoder(bureau)\n",
    "\n",
    "# align to grain of SK_ID_CURR\n",
    "grp = bureau.groupby(['SK_ID_CURR']).agg({\n",
    "        'bureau_CREDIT_ACTIVE_FLAG': ['sum', 'mean'],  \n",
    "        'bureau_CREDIT_ENDDATE_FLAG': ['sum', 'mean'], \n",
    "        'bureau_DAYS_CREDIT_ENDDATE': ['mean', 'max','min', 'var'],\n",
    "        'bureau_DAYS_CREDIT_UPDATE': ['mean', 'max','min','var'], \n",
    "        'bureau_AMT_CREDIT_SUM_DEBT': ['sum', 'mean', 'max','min','var'], \n",
    "        'bureau_AMT_CREDIT_SUM_OVERDUE': ['sum', 'max','min','var', 'mean'], \n",
    "        'bureau_AMT_CREDIT_MAX_OVERDUE': ['mean', 'max','min','var'], \n",
    "        'bureau_AMT_CREDIT_SUM': ['sum', 'max','min','var', 'mean'], \n",
    "        'bureau_AMT_CREDIT_SUM_LIMIT': ['sum', 'max','min','var'], \n",
    "        'bureau_AMT_ANNUITY': ['sum', 'mean', 'max','min','var'],  \n",
    "        'bureau_DAYS_CREDIT': ['mean','max','min','var'], \n",
    "        'bb_STATUS_X_PERCENT': ['mean', 'max','min','var'],\n",
    "        'bb_STATUS_C_PERCENT': ['mean', 'max','min','var'], \n",
    "        'bb_STATUS_1_PERCENT': ['mean', 'max','min','var'], \n",
    "        'bb_MONTHS_BALANCE_size': ['max', 'var','sum', 'mean'],\n",
    "        'bureau_CREDIT_TYPE_Credit card': ['sum'],\n",
    "        'bureau_CREDIT_TYPE_Microloan': ['sum']\n",
    "})\n",
    "grp.columns = pd.Index([e[0] + '_' + e[1] for e in grp.columns.tolist()])\n",
    "df_all_ids = df_all_ids.join(grp, on = ['SK_ID_CURR'], how = 'left')\n",
    "\n",
    "\"\"\"\n",
    "# aggregations separately for active\n",
    "num_aggregations = {\n",
    "        'bureau_DAYS_CREDIT': ['max', 'var', 'mean'], \n",
    "        'bureau_DAYS_CREDIT_ENDDATE': ['min', 'mean'],\n",
    "        'bureau_DAYS_CREDIT_UPDATE': ['mean', 'max','min'], \n",
    "        'bureau_AMT_CREDIT_SUM_LIMIT': ['mean', 'max','min'], \n",
    "        'bb_MONTHS_BALANCE_size': ['sum']\n",
    "    }\n",
    "\n",
    "active = bureau[bureau['bureau_CREDIT_ACTIVE_FLAG'] == 1]\n",
    "grp = active.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "grp.columns = pd.Index([e[0] + \"_\" + e[1] + \"_ACTIVE\" for e in grp.columns.tolist()])\n",
    "df_all_ids = df_all_ids.join(grp, how='left', on='SK_ID_CURR')\n",
    "df_all_ids = fill_na_with(df_all_ids)    \n",
    "\"\"\"\n",
    "df_all_ids['bureau_CREDIT_MINUS_OVERDUE'] = df_all_ids['bureau_AMT_CREDIT_SUM_mean'] - df_all_ids['bureau_AMT_CREDIT_SUM_OVERDUE_mean'] \n",
    "df_all_ids['bureau_MONTHSBALANCE_DIVIDEDBY_DAYSCREDIT'] = df_all_ids['bb_MONTHS_BALANCE_size_mean'] / df_all_ids['bureau_DAYS_CREDIT_mean'] \n",
    "\n",
    "df_all_ids = fill_na_with(df_all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_all_ids.set_index('SK_ID_CURR', inplace = True)\n",
    "\n",
    "train = train.merge(df_all_ids, how='left', left_index = True, right_index = True)\n",
    "test = test.merge(df_all_ids, how='left', left_index = True, right_index = True)\n",
    "\n",
    "train = fill_na_with(train)\n",
    "test = fill_na_with(test)\n",
    "\n",
    "gc.enable()\n",
    "del bb, bureau, df_all_ids, grp\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Source: POS Cash Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos = pd.read_csv('data/POS_Cash_balance.csv')\n",
    "pos = pos.add_prefix('cash_') #add prefix showing source\n",
    "pos = pos.rename(columns = {'cash_SK_ID_CURR': 'SK_ID_CURR'}) #remove prefix from id\n",
    "pos = pos.fillna(0)\n",
    "\n",
    "df_all_ids = pd.DataFrame({'SK_ID_CURR':pos['SK_ID_CURR'].unique()}) #use to align grain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Retrieve most recent row per cash loan (group of SK_ID_CURR and cash_SK_ID_PREV)\n",
    "grp = pos.groupby(['SK_ID_CURR', 'cash_SK_ID_PREV'], as_index=False).first()\n",
    "grp['cash_FUTURE_RATIO'] = grp['cash_CNT_INSTALMENT_FUTURE'] / grp['cash_CNT_INSTALMENT']\n",
    "grp['cash_CONTRACT_STATUS_ACTIVE'] = 0\n",
    "grp['cash_CONTRACT_STATUS_ACTIVE'] = grp['cash_CONTRACT_STATUS_ACTIVE'].mask(grp['cash_NAME_CONTRACT_STATUS'] == 'Active', 1)\n",
    "\n",
    "# align from grain of SK_ID_PREV to grain of SK_ID_CURR\n",
    "grp1 = grp.groupby(['SK_ID_CURR']).agg({\n",
    "            'cash_CONTRACT_STATUS_ACTIVE': ['sum'],\n",
    "            'cash_FUTURE_RATIO': ['min'],\n",
    "            'cash_CNT_INSTALMENT_FUTURE': ['mean', 'max','min'] \n",
    "        })\n",
    "grp1.columns = pd.Index([e[0] + '_' + e[1] for e in grp1.columns.tolist()])\n",
    "df_all_ids = df_all_ids.join(grp1, on = ['SK_ID_CURR'], how = 'left')\n",
    "\n",
    "# Calculates days late\n",
    "pos['cash_SK_DPD_late'] = 0\n",
    "pos['cash_SK_DPD_late'] = pos['cash_SK_DPD_late'].mask(pos['cash_SK_DPD'] >= 1, 1)\n",
    "\n",
    "# Fields calculated at grain of month installment\n",
    "pos, cat_cols = one_hot_encoder(pos)\n",
    "\n",
    "# align to grain of SK_ID_CURR\n",
    "grp = pos.groupby(['SK_ID_CURR']).agg({\n",
    "        'cash_MONTHS_BALANCE': ['min', 'mean'], \n",
    "        'cash_CNT_INSTALMENT': ['max', 'mean'],\n",
    "        'cash_SK_DPD_late': ['mean', 'max'] \n",
    "        })\n",
    "grp.columns = pd.Index([e[0] + '_' + e[1] for e in grp.columns.tolist()])\n",
    "df_all_ids = df_all_ids.join(grp, on = ['SK_ID_CURR'], how = 'left')\n",
    "df_all_ids = fill_na_with(df_all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_all_ids.set_index('SK_ID_CURR', inplace = True)\n",
    "\n",
    "train = train.merge(df_all_ids, how='left', left_index = True, right_index = True)\n",
    "test = test.merge(df_all_ids, how='left', left_index = True, right_index = True)\n",
    "\n",
    "train = fill_na_with(train)\n",
    "test = fill_na_with(test) \n",
    "\n",
    "gc.enable()\n",
    "del pos, df_all_ids, grp\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Source: CC Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cc = pd.read_csv('data/credit_card_balance.csv')\n",
    "cc = cc.add_prefix('cc_') #add prefix showing source\n",
    "cc = cc.rename(columns = {'cc_SK_ID_CURR': 'SK_ID_CURR'}) #remove prefix from id\n",
    "cc = cc.fillna(0)\n",
    "\n",
    "df_all_ids = pd.DataFrame({'SK_ID_CURR':cc['SK_ID_CURR'].unique()}) #use to align grain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cc, cat_cols = one_hot_encoder(cc) \n",
    "\n",
    "# align to grain of SK_ID_CURR\n",
    "grp = cc.groupby(['SK_ID_CURR']).agg({\n",
    "                    'cc_MONTHS_BALANCE': ['min', 'max', 'mean'], \n",
    "                    'cc_AMT_BALANCE': ['mean', 'max','min'], \n",
    "                    'cc_AMT_DRAWINGS_CURRENT': ['mean', 'max','min'], \n",
    "                    'cc_SK_DPD_DEF': ['mean', 'max','min'], \n",
    "                    'cc_CNT_INSTALMENT_MATURE_CUM': ['mean', 'max','min'], \n",
    "                    'cc_AMT_DRAWINGS_ATM_CURRENT': ['mean', 'max','min'], \n",
    "                    'cc_AMT_CREDIT_LIMIT_ACTUAL': ['mean', 'max','min'],\n",
    "                    'cc_AMT_DRAWINGS_ATM_CURRENT': ['mean', 'max','min'],\n",
    "                    'cc_AMT_DRAWINGS_OTHER_CURRENT': ['mean', 'max','min'],\n",
    "                    'cc_AMT_INST_MIN_REGULARITY': ['mean', 'max','min'],\n",
    "                    'cc_AMT_RECEIVABLE_PRINCIPAL': ['mean', 'max','min'],\n",
    "                    'cc_AMT_TOTAL_RECEIVABLE': ['mean', 'max','min']\n",
    "            })\n",
    "grp.columns = pd.Index([e[0] + '_' + e[1] for e in grp.columns.tolist()])\n",
    "df_all_ids = df_all_ids.join(grp, on = ['SK_ID_CURR'], how = 'left')\n",
    "df_all_ids = fill_na_with(df_all_ids)\n",
    "\n",
    "#fields created at grain of SK_ID_CURR\n",
    "df_all_ids['cc_ATM_TO_TOTALDRAW_RATIO'] = df_all_ids['cc_AMT_DRAWINGS_ATM_CURRENT_mean'] / df_all_ids[\n",
    "    'cc_AMT_DRAWINGS_CURRENT_mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_all_ids.set_index('SK_ID_CURR', inplace = True)\n",
    "\n",
    "train = train.merge(df_all_ids, how='left', left_index = True, right_index = True)\n",
    "test = test.merge(df_all_ids, how='left', left_index = True, right_index = True)\n",
    "\n",
    "train = fill_na_with(train)\n",
    "test = fill_na_with(test)\n",
    "\n",
    "gc.enable()\n",
    "del cc, df_all_ids, grp\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Source: Previous Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prevapp = pd.read_csv('data/previous_application.csv')\n",
    "prevapp = prevapp.add_prefix('prevapp_') #add prefix showing source\n",
    "prevapp = prevapp.rename(columns = {'prevapp_SK_ID_CURR': 'SK_ID_CURR'}) #remove prefix from id\n",
    "prevapp = prevapp.fillna(0)\n",
    "\n",
    "df_all_ids = pd.DataFrame({'SK_ID_CURR':prevapp['SK_ID_CURR'].unique()}) #use to align grain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# processing on grain of previous appp\n",
    "prevapp['prevapp_DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace= True)\n",
    "prevapp['prevapp_DAYS_FIRST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "prevapp['prevapp_DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace= True)\n",
    "prevapp['prevapp_DAYS_LAST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "prevapp['prevapp_DAYS_TERMINATION'].replace(365243, np.nan, inplace= True)\n",
    "prevapp['prevapp_ASKED_MINUS_CREDITGIVEN'] = prevapp['prevapp_AMT_APPLICATION'] - prevapp['prevapp_AMT_CREDIT']\n",
    "prevapp['prevapp_CREDIT_MINUS_GOODSPRICE'] = prevapp['prevapp_AMT_CREDIT'] - prevapp['prevapp_AMT_GOODS_PRICE']\n",
    "\n",
    "prevapp, cat_cols = one_hot_encoder(prevapp)\n",
    "\n",
    "# align to grain of SK_ID_CURR\n",
    "grp = prevapp.groupby(['SK_ID_CURR']).agg({\n",
    "                    'prevapp_AMT_ANNUITY': ['mean', 'max','min','var'], \n",
    "                    'prevapp_AMT_APPLICATION': ['mean', 'max','min', 'var'],    \n",
    "                    'prevapp_AMT_DOWN_PAYMENT': ['max', 'mean', 'min'],\n",
    "                    'prevapp_AMT_CREDIT': ['min', 'mean', 'max'], \n",
    "                    'prevapp_ASKED_MINUS_CREDITGIVEN': ['max', 'mean', 'min'], \n",
    "                    'prevapp_CREDIT_MINUS_GOODSPRICE': ['mean','max', 'min'], \n",
    "                    'prevapp_RATE_DOWN_PAYMENT': ['max', 'min'], \n",
    "                    'prevapp_DAYS_DECISION': ['max', 'mean'], \n",
    "                    'prevapp_CNT_PAYMENT': ['mean', 'max', 'min'], \n",
    "                    'prevapp_DAYS_LAST_DUE': ['max', 'mean', 'var'], \n",
    "                    'prevapp_NAME_CONTRACT_TYPE_Consumer loans': ['sum'],\n",
    "                    'prevapp_NAME_CONTRACT_STATUS_Canceled': ['sum'],\n",
    "                    'prevapp_CODE_REJECT_REASON_HC': ['sum'],         \n",
    "                    'prevapp_NAME_PRODUCT_TYPE_x-sell': ['sum','mean'], \n",
    "                    'prevapp_NAME_PRODUCT_TYPE_walk-in': ['sum','mean'],       \n",
    "                    'prevapp_NAME_YIELD_GROUP_middle': ['sum', 'mean'], \n",
    "                    'prevapp_NAME_YIELD_GROUP_high': ['sum', 'mean'],\n",
    "                    'prevapp_NAME_YIELD_GROUP_low_action': ['sum', 'mean'],\n",
    "                    'prevapp_DAYS_FIRST_DRAWING':['mean','max', 'min'],\n",
    "                    'prevapp_DAYS_FIRST_DUE': ['mean','max', 'min'],\n",
    "                    'prevapp_DAYS_LAST_DUE_1ST_VERSION': ['mean','max', 'min'],\n",
    "                    'prevapp_DAYS_TERMINATION': ['mean','max', 'min']\n",
    "                  })\n",
    "grp.columns = pd.Index([e[0] + '_' + e[1] for e in grp.columns.tolist()])\n",
    "df_all_ids = df_all_ids.join(grp, on = ['SK_ID_CURR'], how = 'left')\n",
    "df_all_ids = fill_na_with(df_all_ids)\n",
    "\n",
    "num_aggregations = {'prevapp_AMT_CREDIT': ['min'],\n",
    "                    'prevapp_CNT_PAYMENT': ['sum'],\n",
    "                  }\n",
    "approved = prevapp[prevapp['prevapp_NAME_CONTRACT_STATUS_Approved'] == 1]\n",
    "grp = approved.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "grp.columns = pd.Index([e[0] + \"_\" + e[1] + \"_APPROVED\" for e in grp.columns.tolist()])\n",
    "df_all_ids = df_all_ids.join(grp, how='left', on='SK_ID_CURR')\n",
    "df_all_ids = fill_na_with(df_all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_all_ids.set_index('SK_ID_CURR', inplace = True)\n",
    "\n",
    "train = train.merge(df_all_ids, how='left', left_index = True, right_index = True)\n",
    "test = test.merge(df_all_ids, how='left', left_index = True, right_index = True)\n",
    "train = fill_na_with(train)\n",
    "test = fill_na_with(test)\n",
    "\n",
    "gc.enable()\n",
    "del prevapp, df_all_ids, grp\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Source: Installment Payments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "installpay = pd.read_csv('data/installments_payments.csv')\n",
    "installpay = installpay.add_prefix('installpay_') #add prefix showing source\n",
    "installpay = installpay.rename(columns = {'installpay_SK_ID_CURR': 'SK_ID_CURR'}) #remove prefix from id\n",
    "installpay = installpay.fillna(0)\n",
    "\n",
    "df_all_ids = pd.DataFrame({'SK_ID_CURR':installpay['SK_ID_CURR'].unique()}) #use to align grain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#create new columns on grain of installment payment\n",
    "installpay['installpay_BALANCE_DUE'] = installpay['installpay_AMT_PAYMENT']- installpay['installpay_AMT_INSTALMENT']\n",
    "installpay['installpay_DAYS_DELTA'] = installpay['installpay_DAYS_INSTALMENT']- installpay['installpay_DAYS_ENTRY_PAYMENT']\n",
    "installpay['installpay_DAYS_LATE_FLAG'] = (installpay['installpay_DAYS_DELTA'] < 0).astype(int)\n",
    "installpay['installpay_DAYS_LATE'] = installpay['installpay_DAYS_DELTA']\n",
    "installpay['installpay_DAYS_LATE'] = installpay['installpay_DAYS_LATE'].mask(installpay['installpay_DAYS_DELTA'] > 0, 0)\n",
    "\n",
    "installpay['installpay_PAID_OVER_AMOUNT'] = installpay['installpay_AMT_PAYMENT'] - installpay['installpay_AMT_INSTALMENT']\n",
    "installpay['installpay_PAID_OVER_FLAG'] = (installpay['installpay_PAID_OVER_AMOUNT'] > 0).astype(int)\n",
    "\n",
    "installpay, cat_cols = one_hot_encoder(installpay)\n",
    "\n",
    "# align to grain of SK_ID_CURR\n",
    "grp = installpay.groupby(['SK_ID_CURR']).agg({\n",
    "        'installpay_DAYS_DELTA': ['mean','max','min', 'var'], \n",
    "        'installpay_DAYS_LATE': ['sum','max','min', 'var'], \n",
    "        'installpay_BALANCE_DUE': ['mean','max','min', 'var'], \n",
    "        'installpay_PAID_OVER_AMOUNT': ['mean','max','min'],\n",
    "        'installpay_AMT_PAYMENT': ['sum', 'mean','max','min','var'], \n",
    "        'installpay_DAYS_LATE_FLAG': ['sum', 'mean'],\n",
    "        'installpay_PAID_OVER_FLAG': ['sum', 'mean'] \n",
    "})\n",
    "grp.columns = pd.Index([e[0] + '_' + e[1] for e in grp.columns.tolist()])\n",
    "df_all_ids = df_all_ids.join(grp, on = ['SK_ID_CURR'], how = 'left')\n",
    "\n",
    "df_all_ids = fill_na_with(df_all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_all_ids.set_index('SK_ID_CURR', inplace = True)\n",
    "\n",
    "train = train.merge(df_all_ids, how='left', left_index = True, right_index = True)\n",
    "test = test.merge(df_all_ids, how='left', left_index = True, right_index = True)\n",
    "\n",
    "train = fill_na_with(train)\n",
    "test = fill_na_with(test)\n",
    "\n",
    "gc.enable()\n",
    "del installpay, df_all_ids, grp\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimension Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "drop_these_columns = ['OWN_CAR_AGE', 'CNT_FAM_MEMBERS', 'CNT_CHILDREN']\n",
    "\n",
    "train.drop(drop_these_columns, axis=1, inplace=True)\n",
    "test.drop(drop_these_columns, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_X = train.copy()\n",
    "test_X = test.copy()\n",
    "train_X_labels = traintarget['TARGET']  #this saves the value we want to predict\n",
    "\n",
    "def run_lda_on_regex(train_df, test_df, regex, name, targets):\n",
    "    lda = LinearDiscriminantAnalysis(solver = 'eigen', shrinkage = 'auto')\n",
    "    lda.fit(train_df.filter(regex = regex), targets)\n",
    "    \n",
    "    train_lda = pd.DataFrame(lda.transform(\n",
    "        train_df.filter(regex = regex)), index = train_df.index).add_prefix(name)\n",
    "    train_df = train_df.merge(train_lda, how='left', left_index = True, right_index = True)\n",
    "    \n",
    "    test_lda = pd.DataFrame(lda.transform(\n",
    "        test_df.filter(regex = regex)), index = test_df.index).add_prefix(name)\n",
    "    test_df = test_df.merge(test_lda, how='left', left_index = True, right_index = True)\n",
    "    \n",
    "    train_df.drop(train_df.filter(regex = regex).columns, axis = 1, inplace = True)\n",
    "    test_df.drop(test_df.filter(regex = regex).columns, axis = 1, inplace = True)\n",
    "    \n",
    "    return (train_df, test_df)\n",
    "\n",
    "train_X, test_X = run_lda_on_regex(train_X, test_X,\n",
    "   r'(OCCUPATION_TYPE|FLAG_DOCUMENT|NAME_HOUSING|NAME_FAMILY_STATUS|ORGANIZATION_TYPE|EMERGENCYSTATE_MODE|FONDKAPREMONT_MODE|WALLSMATERIAL_MODE|FLAG_EMAIL)_+', 'LDA_', train_X_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Prep Before Modeing: get_dummies to convert to numeric columns, align columns, scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# all columns must be numeric\n",
    "train_X = pd.get_dummies(train_X)\n",
    "test_X = pd.get_dummies(test_X)\n",
    "\n",
    "# align columns, keeping only columns present in both df\n",
    "train_X, test_X = train_X.align(test_X, join = 'inner', axis = 1)\n",
    "feature_names = train_X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# look for infinities\n",
    "pd.options.display.max_rows = 4000\n",
    "train_X.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(train_X)\n",
    "\n",
    "train_X = scaler.transform(train_X)\n",
    "test_X = scaler.transform(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model Using LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# number of fold cross validation\n",
    "n_folds = 10 #set the number of folds for K-fold validation (better at 10 but training takes to long)\n",
    "folds = KFold(n_splits=n_folds, shuffle=True, random_state=50)\n",
    "\n",
    "# Empty arrays and lists\n",
    "feature_importance = np.zeros(len(feature_names))\n",
    "train_preds = np.zeros(train_X.shape[0])\n",
    "valid_preds = np.zeros(train_X.shape[0])\n",
    "test_preds = np.zeros(test_X.shape[0])\n",
    "valid_scores = []\n",
    "train_scores = []\n",
    "\n",
    "# Iterate through each fold\n",
    "for n_fold, (train_indices, valid_indices) in enumerate(folds.split(train_X)):\n",
    "    # Training data for the fold\n",
    "    train_fold, train_fold_labels = train_X[train_indices, :], train_X_labels[train_indices]\n",
    "    \n",
    "    # Validation data for the fold\n",
    "    valid_fold, valid_fold_labels = train_X[valid_indices, :], train_X_labels[valid_indices]\n",
    "    \n",
    "    # LightGBM classifier with hyperparameters\n",
    "    model = LGBMClassifier(\n",
    "        n_estimators=10000,  #num trees to fit, generalizes model better with more trees, longer training\n",
    "        \n",
    "        learning_rate=0.01,  #default is .1, lower generalize well but longer training. impact each tree on final outcome\n",
    "        subsample=.8,\n",
    "        num_leaves=34,\n",
    "        max_depth=6,  #recommend 4-8, lower this if you think model is over fitting\n",
    "        min_data_in_leaf=200,  #too low results in overfitting, high underfitting (fit accuracy)\n",
    "        reg_alpha=.04, #changed from .1, these should complement - one high and one low\n",
    "        reg_lambda=.07,  #changed from .1, typically 0 to 1\n",
    "        min_split_gain=.03, #control number of useful splits in tree\n",
    "        min_child_weight=1.5,\n",
    "        max_bin=1000   #num buckets, higher more accuracy but slower training and may overfit, results similar at 1200, \n",
    "        )\n",
    "    \n",
    "    # Fit on the training data, evaluate on the validation data\n",
    "    model.fit(train_fold, train_fold_labels, \n",
    "            eval_metric=('auc'), #can also do error, mae (mean absolute error), mse (mean squared error), multi-logloss\n",
    "            eval_set = [(train_fold, train_fold_labels), (valid_fold, valid_fold_labels)], \n",
    "            early_stopping_rounds=100, #reduces number iterations\n",
    "            verbose = False\n",
    "            )\n",
    "    \n",
    "    # Validation preditions\n",
    "    train_preds[train_indices] = model.predict_proba(train_fold, num_iteration=model.best_iteration_)[:, 1]\n",
    "    valid_preds[valid_indices] = model.predict_proba(valid_fold, num_iteration=model.best_iteration_)[:, 1]\n",
    "    \n",
    "    # Testing predictions\n",
    "    test_preds += model.predict_proba(test_X, num_iteration=model.best_iteration_)[:, 1] / folds.n_splits\n",
    "    predictions = test_preds.copy()\n",
    "    \n",
    "    # Display the performance for the current fold\n",
    "    print('Fold %d AUC : %0.6f' % (n_fold + 1, roc_auc_score(valid_fold_labels, valid_preds[valid_indices])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_true = train_fold_labels\n",
    "y_pred = (train_preds[train_indices] > 0.5).astype(int)  \n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "print' --------------- training data ------------------'\n",
    "print\n",
    "print'data accuracy score: ', accuracy_score(y_true, y_pred)\n",
    "print\n",
    "print'confusion matrix:'\n",
    "print(pd.DataFrame(confusion_matrix(y_true, y_pred)))\n",
    "print\n",
    "print'classification report: '\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "y_true = valid_fold_labels\n",
    "y_pred = (valid_preds[valid_indices] > 0.5).astype(int) \n",
    "\n",
    "print' --------------- validation data ------------------'\n",
    "print\n",
    "print'data accuracy score: ', accuracy_score(y_true, y_pred)\n",
    "print\n",
    "print'confusion matrix:'\n",
    "print(pd.DataFrame(confusion_matrix(y_true, y_pred)))\n",
    "print\n",
    "print'classification report: '\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submit = pd.DataFrame()\n",
    "submit['TARGET'] = predictions #connect id with prediction\n",
    "submit.index = test.index\n",
    "print submit.head() \n",
    "submit.to_csv('Predictions_LGBM_1.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 4000\n",
    "feature_import_df = pd.DataFrame({'feature': feature_names, 'importance': model.feature_importances_})\n",
    "\n",
    "def plot_sorted_feature_importance (df):\n",
    "    plt.figure(figsize = (12, 8))\n",
    "    df_sorted = df.sort_values('importance', ascending = False)\n",
    "    import_threshold = 10\n",
    "    print('Number of features with importance greater than ', import_threshold, ' = ', np.sum(df_sorted['importance'] > import_threshold))\n",
    "    df_sorted.head(20).plot(x = 'feature', y = 'importance', kind = 'barh',\n",
    "                     color = 'red', edgecolor = 'k', title = 'Top 20 Feature Importances',);\n",
    "    return df_sorted\n",
    "\n",
    "feature_import_df_sorted = plot_sorted_feature_importance(feature_import_df)\n",
    "print feature_import_df_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation to Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_corr_to_target (df):\n",
    "    target_corr = df.corr()['TARGET']  #target_corr is correlation of specific variables to target \n",
    "    target_corr = target_corr.sort_values()\n",
    "    return target_corr # Correlation to target variable\n",
    "\n",
    "pd.options.display.max_rows = 4000\n",
    "target_corr = find_corr_to_target(train) #can use subset or all\n",
    "print ('Correlation to Target')\n",
    "print target_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation between variables (multicollinearity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_multicollinearity (df, threshold = 0.8):\n",
    "    corrs = df.corr()\n",
    "    above_threshold_vars = {} # dictionary to hold correlated variables\n",
    "\n",
    "    # Record variables that are above threshold.\n",
    "    for col in corrs:  \n",
    "        above_threshold_vars[col] = list(corrs.index[corrs[col] > threshold])\n",
    "\n",
    "    # Track of columns already examined\n",
    "    drop_these_columns = []\n",
    "    drop_these_columns_pairs = []\n",
    "    cols_seen = []\n",
    "    for key, value in above_threshold_vars.items():\n",
    "        cols_seen.append(key)\n",
    "        for x in value:\n",
    "            if x == key:\n",
    "                next\n",
    "            else: # Only want to remove one in a pair\n",
    "                if x not in cols_seen:\n",
    "                    drop_these_columns.append(x)\n",
    "                    drop_these_columns_pairs.append(key)\n",
    "            \n",
    "    drop_these_columns = list(set(drop_these_columns))\n",
    "    return above_threshold_vars, drop_these_columns, threshold\n",
    "\n",
    "# use correlation between variables to identify columns to delete\n",
    "data_subset = train[[]]\n",
    "above_threshold_vars, drop_these_columns, threshold = find_multicollinearity (data_subset, .8)\n",
    "print 'Suggested number of columns to remove with collinearity above', threshold, ': ', len(drop_these_columns)\n",
    "drop_these_columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
